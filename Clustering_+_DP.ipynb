{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8fSsn+WfmrW377ARZ+avn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biruk50/Medium_articles/blob/main/Clustering_%2B_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2Pp1FOOX9-7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque,namedtuple\n",
        "from google.colab import files  # For file upload in Google Colab\n",
        "from hdbscan import HDBSCAN\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "2iCkVmnFdM7o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Item named tuple\n",
        "Item = namedtuple(\"Item\", ['index', 'value', 'weight', 'cluster_id'])\n",
        "\n",
        "# Upload and read file\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "    input_data = file.read().strip()\n",
        "\n",
        "# Parse the input data\n",
        "lines = input_data.split('\\n')\n",
        "first_line = lines[0].split()\n",
        "item_count = int(first_line[0])\n",
        "capacity = int(first_line[1])\n",
        "\n",
        "print(f\"capacity {capacity}\")\n",
        "\n",
        "items = []\n",
        "for i, line in enumerate(lines[1:], start=1):\n",
        "    value, weight = map(int, line.split())\n",
        "    items.append(Item(i, value, weight, -1))  # Initialize with no cluster\n",
        "\n",
        "# Clustering with HDBSCAN\n",
        "min_cluster_size = int(math.log10(item_count)) + 1\n",
        "hdbscan_clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=1)\n",
        "X = np.array([(item.weight, item.value) for item in items])\n",
        "cluster_labels = hdbscan_clusterer.fit_predict(X)\n",
        "\n",
        "# Organize items into clusters and noise\n",
        "cluster_dict: Dict[int, List[Item]] = {}\n",
        "noise_items = []\n",
        "\n",
        "for label, item in zip(cluster_labels, items):\n",
        "    if label == -1:  # Noise point\n",
        "        noise_items.append(item)\n",
        "    else:\n",
        "        if label not in cluster_dict:\n",
        "            cluster_dict[label] = []\n",
        "        cluster_dict[label].append(item._replace(cluster_id=label))\n",
        "\n",
        "# Print cluster and noise information\n",
        "print(f\"\\nClusters number: {len(cluster_dict)}\")\n",
        "\n",
        "print(f\"\\nNoise Items number: {len(noise_items)}\")\n",
        "#print([item.index for item in noise_items])\n",
        "\n",
        "# Initialize knapsack values\n",
        "final_value = 0\n",
        "total_weight = 0\n",
        "taken = [0] * item_count\n",
        "remaining_capacity = capacity\n",
        "\n",
        "top_half_noise_items = noise_items[:int(len(noise_items)//2)]\n",
        "\n",
        "# Calculate value-to-weight ratio for noise items and clusters\n",
        "noise_ratio = sum(item.value for item in noise_items) / sum(item.weight for item in top_half_noise_items) if top_half_noise_items else 0\n",
        "cluster_ratio = sum(sum(item.value for item in cluster) for cluster in cluster_dict.values()) / sum(sum(item.weight for item in cluster) for cluster in cluster_dict.values()) if cluster_dict else 0\n",
        "\n",
        "# Allocate capacity based on ratio comparison\n",
        "noise_capacity =  int(remaining_capacity * ( noise_ratio / (noise_ratio + cluster_ratio)) )\n",
        "\n",
        "print(f\"noise_capacity {noise_capacity}\")\n",
        "\n",
        "# Step 2: Dynamic Programming for Noise Items\n",
        "if top_half_noise_items:\n",
        "    step_size = max(1, noise_capacity // (5 * len(top_half_noise_items)))\n",
        "    noise_columns = noise_capacity // step_size + 1\n",
        "    noise_dp_table = [[0] * (noise_columns + 1) for _ in range(len(top_half_noise_items) + 1)]\n",
        "\n",
        "    for i in range(1, len(top_half_noise_items) + 1):\n",
        "        for j in range(1, noise_columns + 1):\n",
        "            item = top_half_noise_items[i - 1]\n",
        "            column_capacity = j * step_size\n",
        "            current = 0\n",
        "            if item.weight <= column_capacity:\n",
        "                previous_column_index = (column_capacity - item.weight) // step_size\n",
        "                current = item.value + noise_dp_table[i - 1][previous_column_index]\n",
        "            noise_dp_table[i][j] = max(noise_dp_table[i - 1][j], current)\n",
        "\n",
        "    # Backtrack to Identify Selected Noise Items\n",
        "    selected_noise_items = []\n",
        "    remaining_noise_capacity = noise_capacity\n",
        "    for i in range(len(top_half_noise_items), 0, -1):\n",
        "        column_index = remaining_noise_capacity // step_size\n",
        "        if noise_dp_table[i][column_index] != noise_dp_table[i - 1][column_index]:\n",
        "            selected_noise_items.append(top_half_noise_items[i - 1])\n",
        "            remaining_noise_capacity -= top_half_noise_items[i - 1].weight\n",
        "\n",
        "    # Mark selected noise items\n",
        "    for item in selected_noise_items:\n",
        "        taken[item.index - 1] = 1\n",
        "        final_value += item.value\n",
        "        total_weight += item.weight\n",
        "        remaining_capacity -= item.weight\n",
        "\n",
        "print(f\"remaining capacity after noise: {remaining_capacity} \")\n",
        "print(f\"min_cluster_size : { min_cluster_size }\")\n",
        "# Adjust cluster weights and initialize DP table\n",
        "if cluster_dict and remaining_capacity > 0:\n",
        "    adjusted_capacity = remaining_capacity // min_cluster_size\n",
        "    print(f\"adjusted_capacity {adjusted_capacity}\")\n",
        "    step_size = max(1, adjusted_capacity // (5 * len(cluster_dict)))\n",
        "    print(f\"step_size {step_size}\")\n",
        "    columns = adjusted_capacity // step_size +1\n",
        "    print(f\"columns {columns}\")\n",
        "\n",
        "    # Prepare cluster representatives using average value and weight\n",
        "    cluster_representatives = []\n",
        "    for cluster_id, cluster_items in cluster_dict.items():\n",
        "    # Compute average value and weight of the cluster items\n",
        "      avg_value = sum(item.value for item in cluster_items) // len(cluster_items)\n",
        "      avg_weight = sum(item.weight for item in cluster_items) // len(cluster_items)\n",
        "      cluster_representatives.append({\n",
        "        \"index\": cluster_id,\n",
        "        \"value\": avg_value,\n",
        "        \"weight\": avg_weight,\n",
        "        \"cluster_id\": cluster_id\n",
        "      })\n",
        "\n",
        "    # Initialize DP table\n",
        "    dp_table = [[0] * (columns + 1) for _ in range(len(cluster_representatives) + 1)]\n",
        "    # Fill DP table\n",
        "    for i in range(1, len(cluster_representatives) + 1):\n",
        "        for j in range(1, columns + 1):\n",
        "            current_item = cluster_representatives[i - 1]\n",
        "            column_capacity = j * step_size\n",
        "\n",
        "            current = 0\n",
        "            if current_item[\"weight\"] <= column_capacity:\n",
        "                previous_column_index = (column_capacity - current_item[\"weight\"]) // step_size\n",
        "                current = current_item[\"value\"] + dp_table[i - 1][previous_column_index]\n",
        "            dp_table[i][j] = max(dp_table[i - 1][j], current)\n",
        "\n",
        "\n",
        "    # Print DP table (optional)\n",
        "    print(\"\\nDynamic Programming Table:\")\n",
        "    print (f\"columns {len(dp_table[0])}\")\n",
        "\n",
        "    # Backtrack to Identify Selected Clusters\n",
        "    selected_clusters = []\n",
        "    remaining_cap = adjusted_capacity\n",
        "    orginal_cap=adjusted_capacity\n",
        "\n",
        "    for i in range(len(cluster_representatives), 0, -1):\n",
        "        current_column_index = remaining_cap // step_size\n",
        "        if orginal_cap==remaining_cap:\n",
        "          current_column_index +=1\n",
        "\n",
        "        if dp_table[i][current_column_index] != dp_table[i - 1][current_column_index]:\n",
        "            selected_clusters.append(cluster_representatives[i - 1][\"cluster_id\"])\n",
        "            remaining_cap -= cluster_representatives[i - 1][\"weight\"]\n",
        "\n",
        "    print(\"Clusters number: {selected_clusters}\")\n",
        "\n",
        "selected_clusters.sort(key=lambda cid: sum(item.weight for item in cluster_dict[cid]))\n",
        "# Round-robin selection of items from selected clusters\n",
        "selected_cluster_items = {\n",
        "    cluster_id: deque(sorted(cluster_dict[cluster_id], key=lambda item: item.weight))\n",
        "    for cluster_id in selected_clusters\n",
        "}\n",
        "\n",
        "progress = True\n",
        "while remaining_capacity > 0 and progress and any(selected_cluster_items.values()):\n",
        "    progress = False\n",
        "    for cluster_id, queue in selected_cluster_items.items():\n",
        "        if queue:\n",
        "            item = queue.popleft()\n",
        "            if remaining_capacity >= item.weight:\n",
        "                taken[item.index - 1] = 1\n",
        "                final_value += item.value\n",
        "                total_weight += item.weight\n",
        "                remaining_capacity -= item.weight\n",
        "                progress = True\n",
        "\n",
        "# Output results\n",
        "print(\"\\nSelected items (0 = not taken, 1 = taken):\")\n",
        "print(taken)\n",
        "print(f\"Final knapsack value: {final_value}\")\n",
        "print(f\"Total weight: {total_weight}\")\n",
        "print(f\"Remaining capacity after selection: {remaining_capacity}\")\n"
      ],
      "metadata": {
        "id": "LGIBLJIVYXCK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "class Node:\n",
        "    def __init__(self, level, value, weight, bound, taken):\n",
        "        self.level = level  # Current level in decision tree\n",
        "        self.value = value  # Total value so far\n",
        "        self.weight = weight  # Total weight so far\n",
        "        self.bound = bound  # Upper bound of the value\n",
        "        self.taken = taken  # Items taken so far\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.bound > other.bound  # Max-heap for priority queue\n",
        "\n",
        "\n",
        "def calculate_bound(node, capacity, items):\n",
        "    if node.weight >= capacity:\n",
        "        return 0  # Exceeded capacity, bound is 0\n",
        "\n",
        "    bound = node.value\n",
        "    total_weight = node.weight\n",
        "    level = node.level\n",
        "\n",
        "    while level < len(items) and total_weight + items[level].weight <= capacity:\n",
        "        total_weight += items[level].weight\n",
        "        bound += items[level].value\n",
        "        level += 1\n",
        "\n",
        "    if level < len(items):\n",
        "        bound += (capacity - total_weight) * (items[level].value / items[level].weight)  # Fractional value\n",
        "\n",
        "    return bound\n",
        "\n",
        "\n",
        "def branch_and_bound_knapsack(items, capacity):\n",
        "    items = sorted(items, key=lambda x: x.value / x.weight, reverse=True)  # Sort by value-to-weight ratio\n",
        "    pq = []  # Priority queue (max-heap)\n",
        "    root = Node(level=-1, value=0, weight=0, bound=calculate_bound(Node(-1, 0, 0, 0, []), capacity, items), taken=[])\n",
        "    heapq.heappush(pq, root)\n",
        "    max_value = 0\n",
        "    best_taken = []\n",
        "\n",
        "    while pq:\n",
        "        current = heapq.heappop(pq)\n",
        "\n",
        "        if current.bound > max_value and current.level < len(items) - 1:\n",
        "            next_level = current.level + 1\n",
        "\n",
        "            # Branch where we take the item\n",
        "            if current.weight + items[next_level].weight <= capacity:\n",
        "                taken_with = current.taken + [1]\n",
        "                node_with = Node(\n",
        "                    level=next_level,\n",
        "                    value=current.value + items[next_level].value,\n",
        "                    weight=current.weight + items[next_level].weight,\n",
        "                    bound=calculate_bound(Node(next_level, current.value + items[next_level].value,\n",
        "                                               current.weight + items[next_level].weight, 0, []), capacity, items),\n",
        "                    taken=taken_with,\n",
        "                )\n",
        "                if node_with.value > max_value:\n",
        "                    max_value = node_with.value\n",
        "                    best_taken = node_with.taken\n",
        "                heapq.heappush(pq, node_with)\n",
        "\n",
        "            # Branch where we don't take the item\n",
        "            taken_without = current.taken + [0]\n",
        "            node_without = Node(\n",
        "                level=next_level,\n",
        "                value=current.value,\n",
        "                weight=current.weight,\n",
        "                bound=calculate_bound(Node(next_level, current.value, current.weight, 0, []), capacity, items),\n",
        "                taken=taken_without,\n",
        "            )\n",
        "            heapq.heappush(pq, node_without)\n",
        "\n",
        "    return max_value, best_taken\n"
      ],
      "metadata": {
        "id": "2a0qO0RT-vm2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Item named tuple\n",
        "Item = namedtuple(\"Item\", ['index', 'value', 'weight', 'cluster_id'])\n",
        "\n",
        "# Upload and read file\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "    input_data = file.read().strip()\n",
        "\n",
        "# Parse the input data\n",
        "lines = input_data.split('\\n')\n",
        "first_line = lines[0].split()\n",
        "item_count = int(first_line[0])\n",
        "capacity = int(first_line[1])\n",
        "\n",
        "print(f\"capacity {capacity}\")\n",
        "\n",
        "items = []\n",
        "for i, line in enumerate(lines[1:], start=1):\n",
        "    value, weight = map(int, line.split())\n",
        "    items.append(Item(i, value, weight, -1))\n",
        "\n",
        "# Branch and Bound\n",
        "max_value, taken = branch_and_bound_knapsack(items, capacity)\n",
        "print(\"Branch and Bound Solution:\")\n",
        "print(\"Max Value:\", max_value)\n",
        "print(\"Items Taken:\", taken)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t25dAjJF-2vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Item named tuple\n",
        "Item = namedtuple(\"Item\", ['index', 'value', 'weight', 'cluster_id'])\n",
        "\n",
        "# Upload and read file\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "    input_data = file.read().strip()\n",
        "\n",
        "# Parse the input data\n",
        "lines = input_data.split('\\n')\n",
        "first_line = lines[0].split()\n",
        "item_count = int(first_line[0])\n",
        "capacity = int(first_line[1])\n",
        "\n",
        "print(f\"capacity {capacity}\")\n",
        "\n",
        "items = []\n",
        "for i, line in enumerate(lines[1:], start=1):\n",
        "    value, weight = map(int, line.split())\n",
        "    items.append(Item(i, value, weight, -1))  # Initialize with no cluster\n",
        "\n",
        "# Clustering with HDBSCAN\n",
        "min_cluster_size = int(math.log10(item_count)) + 1\n",
        "hdbscan_clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=1)\n",
        "X = np.array([(item.weight, item.value) for item in items])\n",
        "cluster_labels = hdbscan_clusterer.fit_predict(X)\n",
        "\n",
        "# Organize items into clusters and noise\n",
        "cluster_dict: Dict[int, List[Item]] = {}\n",
        "noise_items = []\n",
        "\n",
        "for label, item in zip(cluster_labels, items):\n",
        "    if label == -1:  # Noise point\n",
        "        noise_items.append(item)\n",
        "    else:\n",
        "        if label not in cluster_dict:\n",
        "            cluster_dict[label] = []\n",
        "        cluster_dict[label].append(item._replace(cluster_id=label))\n",
        "\n",
        "# Print cluster and noise information\n",
        "print(f\"\\nClusters number: {len(cluster_dict)}\")\n",
        "\n",
        "print(f\"\\nNoise Items number: {len(noise_items)}\")\n",
        "#print([item.index for item in noise_items])\n",
        "\n",
        "# Initialize knapsack values\n",
        "final_value = 0\n",
        "total_weight = 0\n",
        "taken = [0] * item_count\n",
        "remaining_capacity = capacity\n",
        "\n",
        "\n",
        "noise_items.sort(key=lambda x: x.value / (x.weight), reverse=True)\n",
        "cut_off_index = int(math.log2(len(noise_items)) )\n",
        "print(f\"cut_off_index { cut_off_index }\")\n",
        "\n",
        "selected_noise_items = noise_items[:cut_off_index]\n",
        "for item in selected_noise_items:\n",
        "    if remaining_capacity >= item.weight:\n",
        "          taken[item.index - 1] = 1\n",
        "          final_value += item.value\n",
        "          total_weight += item.weight\n",
        "          remaining_capacity -= item.weight\n",
        "\n",
        "\n",
        "print(f\"remaining capacity after greedy: {remaining_capacity} \")\n",
        "print(f\"min_cluster_size : {min_cluster_size}\")\n",
        "# Adjust cluster weights and initialize DP table\n",
        "if cluster_dict:\n",
        "    adjusted_capacity = remaining_capacity // min_cluster_size\n",
        "    print(f\"adjusted_capacity {adjusted_capacity}\")\n",
        "    step_size = max(1, adjusted_capacity // (4 * len(cluster_dict)))\n",
        "    print(f\"step_size {step_size}\")\n",
        "    columns = adjusted_capacity // step_size +1\n",
        "    print(f\"columns {columns}\")\n",
        "\n",
        "    # Prepare cluster representatives using average value and weight\n",
        "    cluster_representatives = []\n",
        "    for cluster_id, cluster_items in cluster_dict.items():\n",
        "    # Compute average value and weight of the cluster items\n",
        "      avg_value = sum(item.value for item in cluster_items) // len(cluster_items)\n",
        "      avg_weight = sum(item.weight for item in cluster_items) // len(cluster_items)\n",
        "      cluster_representatives.append({\n",
        "        \"index\": cluster_id,\n",
        "        \"value\": avg_value,\n",
        "        \"weight\": avg_weight,\n",
        "        \"cluster_id\": cluster_id\n",
        "      })\n",
        "\n",
        "    # Initialize DP table\n",
        "    dp_table = [[0] * (columns + 1) for _ in range(len(cluster_representatives) + 1)]\n",
        "    # Fill DP table\n",
        "    for i in range(1, len(cluster_representatives) + 1):\n",
        "        for j in range(1, columns + 1):\n",
        "            current_item = cluster_representatives[i - 1]\n",
        "            column_capacity = j * step_size\n",
        "\n",
        "            current = 0\n",
        "            if current_item[\"weight\"] <= column_capacity:\n",
        "                previous_column_index = (column_capacity - current_item[\"weight\"]) // step_size\n",
        "                current = current_item[\"value\"] + dp_table[i - 1][previous_column_index]\n",
        "            dp_table[i][j] = max(dp_table[i - 1][j], current)\n",
        "\n",
        "\n",
        "    # Print DP table (optional)\n",
        "    print(\"\\nDynamic Programming Table:\")\n",
        "    print (f\"columns {len(dp_table[0])}\")\n",
        "\n",
        "    # Backtrack to Identify Selected Clusters\n",
        "    selected_clusters = []\n",
        "    remaining_cap = adjusted_capacity\n",
        "    orginal_cap=adjusted_capacity\n",
        "\n",
        "    for i in range(len(cluster_representatives), 0, -1):\n",
        "        current_column_index = remaining_cap // step_size\n",
        "        if orginal_cap==remaining_cap:\n",
        "          current_column_index +=1\n",
        "\n",
        "        if dp_table[i][current_column_index] != dp_table[i - 1][current_column_index]:\n",
        "            selected_clusters.append(cluster_representatives[i - 1][\"cluster_id\"])\n",
        "            remaining_cap -= cluster_representatives[i - 1][\"weight\"]\n",
        "\n",
        "    print(\"Clusters number: {selected_clusters}\")\n",
        "\n",
        "selected_clusters.sort(key=lambda cid: sum(item.weight for item in cluster_dict[cid]))\n",
        "# Round-robin selection of items from selected clusters\n",
        "selected_cluster_items = {\n",
        "    cluster_id: deque(sorted(cluster_dict[cluster_id], key=lambda item: item.weight))\n",
        "    for cluster_id in selected_clusters\n",
        "}\n",
        "\n",
        "progress = True\n",
        "while remaining_capacity > 0 and progress and any(selected_cluster_items.values()):\n",
        "    progress = False\n",
        "    for cluster_id, queue in selected_cluster_items.items():\n",
        "        if queue:\n",
        "            item = queue.popleft()\n",
        "            if remaining_capacity >= item.weight:\n",
        "                taken[item.index - 1] = 1\n",
        "                final_value += item.value\n",
        "                total_weight += item.weight\n",
        "                remaining_capacity -= item.weight\n",
        "                progress = True\n",
        "\n",
        "# Output results\n",
        "print(\"\\nSelected items (0 = not taken, 1 = taken):\")\n",
        "print(taken)\n",
        "print(f\"Final knapsack value: {final_value}\")\n",
        "print(f\"Total weight: {total_weight}\")\n",
        "print(f\"Remaining capacity after selection: {remaining_capacity}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1xMDvlcZ_MWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Item named tuple\n",
        "Item = namedtuple(\"Item\", ['index', 'value', 'weight', 'cluster_id'])\n",
        "\n",
        "# Upload and read file\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "with open(file_name, 'r') as file:\n",
        "    input_data = file.read().strip()\n",
        "\n",
        "# Parse the input data\n",
        "lines = input_data.split('\\n')\n",
        "first_line = lines[0].split()\n",
        "item_count = int(first_line[0])\n",
        "capacity = int(first_line[1])\n",
        "\n",
        "print(f\"capacity {capacity}\")\n",
        "\n",
        "items = []\n",
        "for i, line in enumerate(lines[1:], start=1):\n",
        "    value, weight = map(int, line.split())\n",
        "    items.append(Item(i, value, weight, -1))  # Initialize with no cluster\n",
        "\n",
        "# Clustering with HDBSCAN\n",
        "min_cluster_size = int(math.log10(item_count)) + 1\n",
        "hdbscan_clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=1)\n",
        "X = np.array([(item.weight, item.value) for item in items])\n",
        "cluster_labels = hdbscan_clusterer.fit_predict(X)\n",
        "\n",
        "# Organize items into clusters and noise\n",
        "cluster_dict: Dict[int, List[Item]] = {}\n",
        "noise_items = []\n",
        "\n",
        "for label, item in zip(cluster_labels, items):\n",
        "    if label == -1:  # Noise point\n",
        "        noise_items.append(item)\n",
        "    else:\n",
        "        if label not in cluster_dict:\n",
        "            cluster_dict[label] = []\n",
        "        cluster_dict[label].append(item._replace(cluster_id=label))\n",
        "\n",
        "# Print cluster and noise information\n",
        "print(f\"\\nClusters number: {len(cluster_dict)}\")\n",
        "\n",
        "print(f\"\\nNoise Items number: {len(noise_items)}\")\n",
        "#print([item.index for item in noise_items])\n",
        "\n",
        "# Initialize knapsack values\n",
        "final_value = 0\n",
        "total_weight = 0\n",
        "taken = [0] * item_count\n",
        "remaining_capacity = capacity\n",
        "\n",
        "\n",
        "noise_items.sort(key=lambda x: x.value / (x.weight), reverse=True)\n",
        "top_noise_half = noise_items[:int(len(noise_items)* 0.1)]\n",
        "\n",
        "print(f\"min_cluster_size : {min_cluster_size}\")\n",
        "# Adjust cluster weights and initialize DP table\n",
        "if cluster_dict:\n",
        "    adjusted_capacity = capacity\n",
        "    print(f\"adjusted_capacity {adjusted_capacity}\")\n",
        "    step_size = max(1, adjusted_capacity // (4 * len(cluster_dict)))\n",
        "    print(f\"step_size {step_size}\")\n",
        "    columns = adjusted_capacity // step_size +1\n",
        "    print(f\"columns {columns}\")\n",
        "\n",
        "    # Prepare cluster representatives using average value and weight\n",
        "    cluster_representatives = []\n",
        "    for cluster_id, cluster_items in cluster_dict.items():\n",
        "    # Compute average value and weight of the cluster items\n",
        "      avg_value = sum(item.value for item in cluster_items) // len(cluster_items)\n",
        "      avg_weight = sum(item.weight for item in cluster_items) // len(cluster_items)\n",
        "      cluster_representatives.append({\n",
        "        \"index\": cluster_id,\n",
        "        \"value\": avg_value,\n",
        "        \"weight\": avg_weight,\n",
        "        \"cluster_id\": cluster_id\n",
        "      })\n",
        "\n",
        "      # Treat each noise item as its own cluster\n",
        "    start_num=len(cluster_representatives) +1\n",
        "    noise_cluster_dict = {}  # Create a separate dictionary for noise items\n",
        "\n",
        "    for noise_item in top_noise_half:\n",
        "      cluster_representatives.append({\n",
        "        \"index\": noise_item.index,\n",
        "        \"value\": noise_item.value,\n",
        "        \"weight\": noise_item.weight,\n",
        "        \"cluster_id\": start_num  # Assign a special cluster ID for individual noise items\n",
        "        })\n",
        "      noise_cluster_dict[start_num] = [noise_item]\n",
        "      start_num+=1\n",
        "\n",
        "    # Initialize DP table\n",
        "    dp_table = [[0] * (columns + 1) for _ in range(len(cluster_representatives) + 1)]\n",
        "    # Fill DP table\n",
        "    for i in range(1, len(cluster_representatives) + 1):\n",
        "        for j in range(1, columns + 1):\n",
        "            current_item = cluster_representatives[i - 1]\n",
        "            column_capacity = j * step_size\n",
        "\n",
        "            current = 0\n",
        "            if current_item[\"weight\"] <= column_capacity:\n",
        "                previous_column_index = (column_capacity - current_item[\"weight\"]) // step_size\n",
        "                current = current_item[\"value\"] + dp_table[i - 1][previous_column_index]\n",
        "            dp_table[i][j] = max(dp_table[i - 1][j], current)\n",
        "\n",
        "\n",
        "    # Print DP table (optional)\n",
        "    print(\"\\nDynamic Programming Table:\")\n",
        "    print (f\"rows {len(dp_table)}\")\n",
        "    print (f\"columns {len(dp_table[0])}\")\n",
        "\n",
        "    # Backtrack to Identify Selected Clusters\n",
        "    selected_clusters = []\n",
        "    remaining_cap = adjusted_capacity\n",
        "    orginal_cap=adjusted_capacity\n",
        "\n",
        "    for i in range(len(cluster_representatives), 0, -1):\n",
        "        current_column_index = remaining_cap // step_size\n",
        "        if orginal_cap==remaining_cap:\n",
        "          current_column_index +=1\n",
        "\n",
        "        if dp_table[i][current_column_index] != dp_table[i - 1][current_column_index]:\n",
        "            selected_clusters.append(cluster_representatives[i - 1][\"cluster_id\"])\n",
        "            remaining_cap -= cluster_representatives[i - 1][\"weight\"]\n",
        "\n",
        "    print(\"Clusters number: {selected_clusters}\")\n",
        "\n",
        "# Fix sorting and round-robin logic to handle both cluster_dict and noise_cluster_dict\n",
        "selected_clusters.sort(\n",
        "    key=lambda cid: sum(item.weight for item in (cluster_dict.get(cid, []) or noise_cluster_dict.get(cid, [])))\n",
        ")\n",
        "\n",
        "# Prepare selected_cluster_items from both cluster_dict and noise_cluster_dict\n",
        "selected_cluster_items = {\n",
        "    cluster_id: deque(sorted(\n",
        "        cluster_dict.get(cluster_id, noise_cluster_dict.get(cluster_id, [])),\n",
        "        key=lambda item: item.weight\n",
        "    ))\n",
        "    for cluster_id in selected_clusters\n",
        "}\n",
        "\n",
        "progress = True\n",
        "while remaining_capacity > 0 and progress and any(selected_cluster_items.values()):\n",
        "    progress = False\n",
        "    for cluster_id, queue in selected_cluster_items.items():\n",
        "        if queue:\n",
        "            item = queue.popleft()\n",
        "            if remaining_capacity >= item.weight:\n",
        "                taken[item.index - 1] = 1\n",
        "                final_value += item.value\n",
        "                total_weight += item.weight\n",
        "                remaining_capacity -= item.weight\n",
        "                progress = True\n",
        "\n",
        "# Output results\n",
        "print(\"\\nSelected items (0 = not taken, 1 = taken):\")\n",
        "print(taken)\n",
        "print(f\"Final knapsack value: {final_value}\")\n",
        "print(f\"Total weight: {total_weight}\")\n",
        "print(f\"Remaining capacity after selection: {remaining_capacity}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H0dn94jGfSHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}